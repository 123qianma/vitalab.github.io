---
layout: review
title:  "Deep Learning at Calcul Québec"
tags:   deep-learning machine-learning
author: Carl Lemaire
pdf:    https://docs.google.com/presentation/d/11fJALWNV-_4hporocEKNlMVIMalI0QuvXmozzsrnD14/edit?usp=sharing
---

Here is a tutorial on how to use deep learning frameworks on _Hélios_, one of _Calcul Québec_'s GPU servers. The procedure for _Hadès_ should be almost identical.

## 1. Should you do it?

If you're not working with very small networks such as AlexNet, the answer is most likely **yes**. I've measured the speedup on the SSD ([Single-Shot multibox Detector]({% post_url deep-learning/2017-02-28-single-shot-detector-ssd %})) code I'm using:

![](/tutorial/images/helios/ssd-speedup.png)

To conclude this section:

![](/tutorial/images/helios/do-it.gif)

## 2. Sign up

1. Ask your supervisor his Calcul Canada identifier (e.g. `pje-224-01`), and his project identifier (e.g. `pje-224-aa`)
2. Sign up on Calcul Canada
3. Sign up on Calcul Québec
4. Wait... (access denied while your account is set up)
5. Login through SSH (use Calcul Québec credentials):

```
ssh username@helios.calculquebec.ca
```

You will arrive on a _login node_. This is not where you run code, but you have internet access. Compute nodes don't have internet access.

## 3. Setup your environment

1. Get your code. Use `scp` or git. To use git, you need to load it: `module load apps/git`.
2. Create a virtualenv and install your requirements. You will need to load python: `module load apps/python/python3.5`.
3. Run your code to make sure you have all dependencies (note that you won't have GPUs).

## 4. Transfer your dataset

The simplest option to send your data to Hélios is to "pipe tar into ssh". You should put your data into the shared storage of the project, accessible using `$RAP`.

```bash
tar czf - <files> | ssh user@host "mkdir $RAP/data/mydataset && cd $RAP/data/mydataset && tar xvzf -"
```

This will transfer all your files as one big file, but upon arrival they will be split back. `scp` is known to be slow when transferring lots of small files. Another option is GLOBUS, which is tailored for big data transfers ([read more](https://wiki.calculquebec.ca/w/Globus/fr)).

**If your dataset contains tons of small files** (e.g. image classification), **you should `tar` them into one big file** [^1]. Then, when you run a task, move the data to your compute node and uncompress it. You can uncompress inside `$RAMDISK`, a disk mounted in RAM!

```bash
tar -cf mydataset.tar mydataset/*  # aggregate without compressing
# Do the following DURING A TASK
mkdir $RAMDISK/mydataset
tar -xf mydataset.tar $RAMDISK/mydataset
```

### Summary of storage resources (Hélios)

|                         | Space  | Speed             | Location    | Duration                   |
|-------------------------|--------|-------------------|-------------|----------------------------|
| Storage of the project  | 1 TB   | ~10 GB/s (shared) | `$RAP`      | Persistent (but no backup) |
| Local storage on node   | 330 GB | ~1 GB/s           | `$LSCRATCH` | Erased after task          |
| RAM disk on node        | 12 GB  | ~10 GB/s          | `$RAMDISK`  | Erased after task          |

<br/>

## 5. Request an interactive task

Before writing script for submitting a task, you should try your command in an **interactive task**. There are a few options you must specify:

* **Project name**, e.g. `pje-224-aa`
* **Resources**; number of nodes, number of GPUs. There are 16 GPUs per "k80" node; each have 12 GB RAM. For each 2 GPUs, you get 3 CPU cores.
* **Maximum time**; time after which your task will be killed. For an interactive task, I suggest 30 min. Thus you will not wait in line for too long.
* **Type of GPU**: ask for a "k80" node! Else you will be sad on a "k20" node where GPUs have 5 GB :(

Example:
```bash
msub -I -A pje-224-aa -l nodes=1:gpus=4 -l walltime=00:30:00 -l feature=k80
```

You will wait a minute or two, and then you'll arrive in a shell. You'll be on a compute node; no internet access, but GPUs! You need to setup this shell; then you can try your code. There are many modules you can load[^2].

```bash
module load cuda/8.0.44 libs/cuDNN/6 apps/python/3.5.0
source venv/bin/activate
./move_dataset_to_node.sh
cd mycode
python train.py
```

Take note the correct sequence of commands.

## 6. Submit a task using a script

```bash
#!/bin/bash
#PBS -A pje-224-aa
#PBS -l walltime=12:00:00
#PBS -l nodes=1:gpus=8
#PBS -l feature=k80
#PBS -m bea
#PBS -r n
 
module load cuda/8.0.44 libs/cuDNN/6 apps/python/3.5.0
source venv/bin/activate
./move_dataset_to_node.sh
cd mycode/
python train.py
```

Note: the `-m` command is for getting email notifications (b: begin, e: end, a: abort). The `-r n` command means that the task should not be restarted after of a power outage.

Save this script as `pouding.sh` (or any other name). Submit the task using:

```bash
msub pouding.sh
```

You will wait in line for longer if you request more time, more resources, or if your script is not named "pouding". The Standard output and Standard error will be written to files, in the directory where you submitted the task.

## More info

Please go to <http://wiki.calculquebec.ca>. Everything you need is there.

<br/><br/>

---

<br/>

[^1]: On Hélios, a project can have a maximum of 500,000 files. This is not related to the space they occupy. Supercomputer storage is optimized for large files (1 MB or more).

[^2]: To list available modules: `module avail`. To list available python versions: `module avail apps/python`. [There is a module for Tensorflow](https://wiki.calculquebec.ca/w/Tensorflow).