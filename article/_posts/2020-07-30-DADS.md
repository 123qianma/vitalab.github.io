---
layout: review
title: "Dynamics-Aware Unsupervised Discovery of Skills"
tags: reinforcement unsupervised
author: "Antoine Th√©berge"
cite:
    authors: "Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman"
    title:   "Dynamics-Aware Unsupervised Discovery of Skills"
    venue:   "ICLR 2020"
pdf: "https://arxiv.org/pdf/1907.01657.pdf"
---


# Highlights

- Unsupervised "skill" discovery without extrinsic rewards
- Planning in "skill" space allows to generalize to unseen tasks

# Introduction

Model-based reinforcement learning (MBRL) tries to learn the dynamics of the environment as well as a policy that plans the optimal course of action using the learned dynamics. This allows the polci to be learned in more sample-efficient way as well as making able to generalize to new tasks.

However, generalizing to new state distributions remains an open problem, as exploring the full state-space and learning the full dynamics for high-dimensional systems is generally infeasible. The authors want to retain the flexibility of MBRL by using the simplicity of model-free RL to learn low-level behaviors than can be leveraged at test time.  

# Methods

The authors want to learn the dynamics for a large and diverse set of skills, which allows them to to use model-based planning in the _skill space_ for solving new tasks.

## Unsupervised skill discovery

Using the common MDP $$M = (S, A, p, r)$$, they want to learn a skill-conditioned policy $$\pi(a \vert s,z)$$ with $$z \in \mathbb(Z)$$ than can be sampled from a prior $$p(z)$$. To plan in the _skill-space_, the authors also propose to learn the dynamics for skills $$q_{\theta}(s'{\vert}s,z)$$, which will be used to plan at test time.

![](/article/images/DADS/fig2.jpeg)

To discover skills, the authors propose to maximise the mutual information (MI) between the next state $$s'$$ and the skill $$z$$ conditioned on the current state $$s$$ s.t.

![](/article/images/DADS/eq1-2.jpeg)

Maximizing the MI corresponds to maximizing the diversity of skills in the latent space $$Z$$ (therefore maximizing the relevancy of skills) while making the transitions for a given $$z$$ predictable (thus making the learning process stable).

Because the actual dynamics are unknown but can be sampled, maximizing (2) is intractable, but the objective can atleast be lower-bounded by

![](/article/images/DADS/eq4.jpeg)

Which leads to the following gradient update for $$q_{\theta}$$

![](/article/images/DADS/eq5.jpeg)

which corresponds to maximizing the likelihood of sample from $$p$$ under $$q_{\theta}$$.

To make use of the learned dynamics, the authors propose to train a policy using any model-free RL algorithm using the following instrinsic reward function

![](/article/images/DADS/eq6.jpeg)

with $$L$$ the number of samples drawn from $$p(z)$$. Again, the reward function encourages the policy to produce transitions that are predictable under $$q_\theta$$ as well as diversified.

In implementation, the authors use the Soft Actor-Critic algorithm to train their policy, and their prior $$p(z) = U(-1, 1)^D$$ where $$D$$ is the dimentionality of the input space.

## Planning in action space

## Data


# Results


# Conclusions


# Remarks


# References

[^1]: Reference about something
