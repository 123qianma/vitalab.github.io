---
layout: review
title: "Self-Attention Generative Adversarial Networks"
tags: deep-learning GAN
author: "Philippe Spino"
cite:
    authors: "Han Zhang Ian Goodfellow Dimitris Metaxas Augustus Odena"
    title:   "Self-Attention Generative Adversarial Network (SAGAN)"
    venue:   "Arxiv"
pdf: "https://arxiv.org/pdf/1805.08318.pdf"
---

# Highlights

- Self-Attention mechanisms coming from the Transformer layers, but applied to Deep Convolution Auto-Encoder Networks and Generative Adversarial Networks.
- Usage of Fréchet Inception Distance[^1] as accuracy measure for generated images.

# Introduction
This newly introduce type of GAN is base on networks allowing attention-driven, long-range dependency modeling and applies it to generation tasks. It has been observed that convolutional GANs have much more difficulty in modeling some image classes than other when trained on multi-class datasets. The goal of SAGAN is to allows the network to capture geometric and/or structural patterns that occurs consistently in some classes (for example, generated images of dogs not having defined paws, but have realistic fur).

# Problems with training convolutional GANs

Despite their greate success in various image generation tasks, the training of GANs are known to be unstable and sensitive to the choices of hyper-parameters (extreme dependency) while training.

# Usage of attention mechanisms

Coming from Attention models, attention mechanisms have become an integral part of models that must capture global dependencies of the data. the convolution processes the information in a local neightborhood, thus loosing the long-range dependencies in images.

the image features from the previous hidden layer is $x \in \R^{C \times N}$ are first transformed into two freature spaces f,g to calculate the attention, where $f(x) = W_f x$ and $g(x) = W_g X$. Then using those variables, they do a softmax on the output such:
![](/article/images/SAGAN/eq1.png)
to indicate the extent to which the model attends to the $i^{th}$ location when synthesizing the $j^{th}$ region.

This sums up to the output of a hidden layer to be $o = (o_1, o_2, ... o_N) \in \R^{C\times N}$ where 
![](/article/images/SAGAN/eq2.png)

All the $W$ are learnable weight matrices implemented as $1\times 1$ convolutions. For the $\beta$ variable, the channel number is always decrease by 8. ($\bar{C} = C/8$) for memory optimisation (this does not cause performance loss)

Both the discriminant and the generator have the self-attention mechnism applied to them.

# Architecture of a Convolutional layer

![](/article/images/SAGAN/fig2.png)

# Training methods

Alternating fashion by minimizing the hinge version of the adversarial loss.
![](/articles/images/SAGAN/eq4.png)

- Usage of spectral norm for both Discriminant and Generator
- Imbalanced learning rate for generator and discriminant updates using two time-scale update rule.
- Accuracy measured with
  - Fréchet Inception Distance(FID)
  - Inception score (KL divergence between conditional class distribution and marginal class distribution)

# Results

It was trained using the ImageNet
![](/article/images/SAGAN/tab1.png)
![](/article/images/SAGAN/tab2.png)

# Remarks

- $\textbf{This was taken from the article repo}$: The current batch size is 64x4=256. Larger batch size seems to give better performance. But it might need to find new hyperparameters for G&D learning rate. Note: It usually takes several weeks to train one million steps.
- There is no mention of how big was the training dataset based on ImageNet. The only thing they mention is that they used LSVRC2012 dataset.

# References

[^1]: GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium (https://arxiv.org/abs/1706.08500)
