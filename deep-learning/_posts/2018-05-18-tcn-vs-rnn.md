---
layout: review
title: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling
tags: deep-learning CNN LSTM RNN
author: "Daniel JÃ¶rgens"
cite:
    authors: "Shaojie Bai, J. Zico Kolter, Vladlen Koltun"
    title:   "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
    venue:   "arxiv.org"
pdf: "https://arxiv.org/abs/1803.01271.pdf"
---

### Motivation

The authors observe that traditionally, recurrent architectures are "the default" choice for sequence modelling.
However, they list a few studies in which "convolutional architectures can reach state-of-the-art accuracy".
From that, they conclude that it would be worthwhile to investigate **if "convolutional sequence modeling" is
"confined to to specific application domains" or whether they depict a general alternative to recurrent
neural networks**.

### Methodology

 * define a "generic temporal convolutional network (TCN) architecture"

   (in order to "distill the best practices in convolutional network design into a simple architecture")
   
 * use common RNN benchmarks to compare TCN against "canonical" RNN, GRU, LSTM
 
### Temporal Convolutional Networks

**essentials**

 * design for mapping f: x_0, ..., x_T -> y_0, ..., y_T
 * causal convolutions, i.e. convolution at time t* considers only inputs at t<t*
 * dilated convolution
 * residual connections

<img src="/deep-learning/images/tcn-vs-rnn/tcn1.png" width="500">

<img src="/deep-learning/images/tcn-vs-rnn/tcn2.png" width="300">
<img src="/deep-learning/images/tcn-vs-rnn/tcn3.png" width="300">

**mentioned advantages compared to recurrent approaches**

 * parallelism
 * flexible receptive field size
 * stable gradients
 * low memory requirement for training (comment: the same as in testing)
 * variable length inputs (comment: not really, see 'disadvantages')

**mentioned disadvantages compared to recurrent approaches**

 * high memory load during evaluation (whole sequence is kept; comment: the same as in training)
 * size of "history" is static, i.e. chosen for particular problem

### Experimental results

![](/deep-learning/images/tcn-vs-rnn/results1.png)

**comment** For several of the metrics it is difficult to judge if the reported differences are large or not.

<img src="/deep-learning/images/tcn-vs-rnn/results2.png" width="700">

**comments**
 * Why do they report "Testing loss" and not validation loss if they talk about convergence here?
 * What is the criterion for convergence? (EURNN in (b) does not seem to have converged.)
 * Hyperparameters could be non-optimal e.g. for LSTM in (b): 1 hidden layer of size 50.

![](/deep-learning/images/tcn-vs-rnn/seqlength.png)

**comments**
 * The size of networks is fixed to 10K.

### Weaknesses

 * For comparing the network sizes, the authors fix the number of weights per network.
   However, since the recurrent units inherently have more weights, the convolutional network
   can arrive at very different (e.g. much deeper) architectures.
   This actually weakens the argument of having optimised the hyperparameters of the recurrent architectures.

 * Why are recurrent networks restricted to have no more than 3 layers?

 * It seems to be a non-optimal study design for drawing general conclusions on the tested networks' performances.
   The recurrent networks seem to be restricted in their optimisation (as explained in the previous bullet),
   which compromises their achievable performance.
   On the other hand, the TCNs are **not** systematically tuned ('... We also empirically find ...').
   However, this 'minimal tuning' should **not** suggest that with better tuning the TCNs could be even better.
   
### Final comment

* Section 5.1. Synopsis of Results

   'With this caveat, the results strongly suggest that the generic TCN architecture with minimal tuning outperforms
   canonical recurrent architetures across a broad variety of sequence modelling tasks that are commonly used
   to benchmark the performance of recurrent architectures.'

   **This does not convice me.**   

## Critical comments

https://www.reddit.com/r/MachineLearning/comments/82cdeq/r_an_empirical_evaluation_of_generic/