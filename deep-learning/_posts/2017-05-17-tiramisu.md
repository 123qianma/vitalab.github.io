---
layout: review
title:  "Tiramisu-Net: The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation"
tags:   deep-learning CNN segmentation
author: Clément Zotti
pdf:   "https://arxiv.org/pdf/1611.09326.pdf"

cite:
  authors: "S.Jégou, M.Drozdzal, D.Vasquez, A.Romero, Y.Bengio"
  title:   "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation"
  venue:   "arXiv 2016"
---

This papers implement all the well-known advances in deep learning segmentations methods and make it work together on these datasets [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/){:target="_blank"} and [Gatech](http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/){:target="_blank"}.

The proposed network implements the following things:

* Dense blocks
* U-Net architecture

## Dense blocks

The dense block provides a low increase of parameters while keeping a good feature extraction.

<div align="middle">
     <img src="/deep-learning/images/tiramisu/dense_blocks.png">
</div>

## U-Net

The U-Net is well-known to have a good performance on various image segmentation datasets.

## Architecture

The network architecture is a straight U-Net with more convolution stages in each step.
<div align="middle">
     <img src="/deep-learning/images/tiramisu/architecture.png">
</div>

In the following table, $$m$$ represents the number of feature maps output, DB means dense block, TD means transition down and TU means transition up.

### Layers

Here is the internal description of all the blocks defined above.

<div align="middle">
     <img src="/deep-learning/images/tiramisu/layers.png">
</div>


## Results

Overall the model is as accurate than the other state-of-the-art models with having far less parameters (100x less for some of them).

### Issues

Even if the model has fewer parameters than the others models, you should keep in mind that it still has far more layers than a regular U-Net so the memory footprint is huge when you want to train it.

If you run it on theano you should use some `THEANO_FLAGS` like `optimizer_including=fusion` to decrease the memory footprint and use a smaller batch_size.

#### Implementations

This is the `Theano/Lasagne` implementations of the paper authors [FC-DenseNet](https://github.com/SimJeg/FC-DenseNet/){:target="_blank"}.

And this is my notebook contribution with `Keras` [implementation](https://gist.github.com/czotti/b1e34c23a92e64490be83f3b8908bdbe){:target="_blank"}.
