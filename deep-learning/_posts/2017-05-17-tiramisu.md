---
layout: review
title:  "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation"
tags:   deep-learning CNN segmentation
author: Clément Zotti
pdf:   "https://arxiv.org/pdf/1611.09326.pdf"

cite:
  authors: "S.Jégou, M.Drozdzal, D.Vasquez, A.Romero, Y.Bengio"
  title:   "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation"
  venue:   "arXiv 2016"
---

This papers implement all the well known advanced in the image segmentation community and make it work together.

List of the methods used:

* Dense blocks
* U-Net architecture
* Using IOU loss


## Dense blocks

The dense block provide a low increase of parameters when keeping a good feature extraction.

<div align="middle">
     <img src="/deep-learning/images/tiramisu/dense_blocks.png">
</div>

## U-Net

The U-Net is well known to have a good performance on various images segmentation dataset.

## Architecture

The network architecture is a straight U-Net with more stage of comvolution in each steps.
<div align="middle">
     <img src="/deep-learning/images/tiramisu/architecture.png">
</div>

In table, $$m$$ represent the number of feature maps output, DB mean dense block, TD mean transition down and TU mean transition up.

### Layers

Here is the internal description of all the blocks defined above.

<div align="middle">
     <img src="/deep-learning/images/tiramisu/layers.png">
</div>


## Results

Overall the model if performing as close as the other state-of-the-art model with far less parameters (100x less for some of them).

### Issues

Even if the model have far less parameters than the others you should keep in mind that he still have far more layers than a regular U-Net so the memory foot print is huge when you want to train it.

If you run it on theano you should use some `THEANO_FLAGS` like `optimizer_includiin=fusion` to decrease the memory foot print and use a smaller batch_size.

