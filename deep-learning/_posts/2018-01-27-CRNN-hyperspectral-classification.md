---
layout: review
title:  "Convolutional Recurrent Neural Networks for Hyperspectral Data Classification"
tags:   hyperspectral, CNN, RNN, classification, deep-learning
author: Charles Authier
pdf:    http://www.mdpi.com:8080/2072-4292/9/3/298/pdf
cite:
  authors: "Hao Wu, Saurabh Prasad"
  title:   "Convolutional Recurrent Neural Networks for Hyperspectral Data Classification"
  venue:   "Remote Sens. 2017"
---

### Description
Hyperspectral data, which capture spectral information of objects over the RGB, have played a key role in remote sensing data analysis. With the modern advanced hyperspectral sensors are able to collect hyperspectral images of very high resolution, from which we can collect a large number of labeled pixels for training deep neural networks.

One-dimensional CNNs are already used to extract spectral features for hyperspectral data in some papers. Two-dimensional CNNs had been employed to extract features for hyperspectral data in other.CNNs can also be used for scene classification of high-resolution remote sensing RGB images.

In a paper of 2016, an RNN was used for land cover change detection on multi-spectral images. Normally RNN and LSTM networks have been used for time series data analysis, such as speech recognition, machine translation, etc. But the LSTM network is a special type of RNN, which is able to capture very long-term dependencies embedded in sequence data like the spectral information in hyperspectral data.

So the idea in this paper is to combine the better of the RNN with a CNN. To be able to make a better spatial classification with all the spectral information indisposition. The CRNN is a hybrid of convolutional and recurrent neural networks. It is composed of several convolutional (and pooling) layers followed by a few recurrent layers (Figure 5). First, the convolutional layers are able to extract middle-level, abstract and locally invariant features from the input sequence. The pooling layers help reduce computation and control overfitting. After the recurrent layers extract contextual information from the feature sequence generated by the previous convolutional layers. Contextual information captures the dependencies between different bands in the hyperspectral sequence.

<img src="/deep-learning/images/hyp_crnn/cnn_hyp.png" width="300"><img src="/deep-learning/images/hyp_crnn/rnn_hyp.png" width="400">
<img src="/deep-learning/images/hyp_crnn/crnn_hyp.png" width="700">

To help in the classification, because the neural networks will conserve the spectral features for each pixel independently, the authors add a linear opinion pools (LOP) in the system. Based on spatially neighboring pixels tend to have similar categories. In smoothing out noisy predictions due to noisy data or outliers.

<img src="/deep-learning/images/hyp_crnn/shema.png" width="700">

### DataSets
The University of Houston (UH) hyperspectral image (144 bands).

<img src="/deep-learning/images/hyp_crnn/dataset_UH.png" width="700">

Indian Pines (180 bands).

<img src="/deep-learning/images/hyp_crnn/dataset_IP.png" width="700">

### Results
To verify the accuracy, they wanted to know if the number of samples will change a lot the average accuracy. So they use 50, 100 and 200 samples for each class in Houston University dataset.
For the Indian Pines dataset, 100, 200 and 300 samples for each class.

<img src="/deep-learning/images/hyp_crnn/results_crnn.png" width="700">

At the same time, the validation losses all converged to a low level, and the validation accuracies all reached a high number. With a very low noise.

<img src="/deep-learning/images/hyp_crnn/noise_crnn.png" width="700">

**Strong points**
*  Recurrent layers can handle variable-length input sequences.
*  the Low number of images needed.
*  Easy to extract information in the features layers.

### In the future
They want to explore the semi-supervised deep learning techniques for hyperspectral image classification.
