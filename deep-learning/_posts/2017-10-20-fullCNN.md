---
layout: review
title: "Full-CNN : Striving for Simplicity: The All Convolutional Net"
tags: deep-learning CNN classification essentials layers
author: "Pierre-Marc Jodoin"
cite:
    authors: "J. T. Springenberg, A. Dosovitskiy, T. Brox and M. Riedmiller"
    title:   "Striving for Simplicity: The All Convolutional Net"
    venue:   "ICLR Workshop 2015"
pdf: "https://arxiv.org/pdf/1412.6806.pdf"
---

## Summary

This paper has two main contributions : 
- they show that replacing MaxPooling with convolution layers of stride > 1 gives better results 
- They proposed a guided backpropagation method to visualize the influence of each neuron.

## Replacing MaxPool with ConvLayers of stride > 1

They took 3 basic architectures (Model A, B and C)

![](/deep-learning/images/fullCNN/sc01.png)

and tested 3 different modifications :

![](/deep-learning/images/fullCNN/sc02.png)

Results show that ALL-CNN + model C give best results againts other configurations

![](/deep-learning/images/fullCNN/sc06.png)

while being very competitive againts other similar approaches

![](/deep-learning/images/fullCNN/sc05.png)

## Guided backpropagation

The idea is to use a double relu while backpropagating gradients.

![](/deep-learning/images/fullCNN/sc03.png)
![](/deep-learning/images/fullCNN/sc04.png)
