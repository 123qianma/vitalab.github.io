---
layout: review
title:  "DenseNet : Densely Connected Convolutional Networks"
tags:   deep-learning CNN classification
author: Carl Lemaire
pdf:    https://arxiv.org/pdf/1608.06993.pdf
cite:
  authors: "Gao Huang, Zhuang Liu, Kilian Q Weinberger, Laurens van der Maaten"
  title:   "Densely connected convolutional networks"
  venue:   "arXiv preprint arXiv:1608.06993"
---

DenseNet is a CNN architecture that includes _dense blocks_. In a dense block, each layer has access to the feature maps of all preceding layers. Figure 1 shows a dense block.

![](/deep-learning/images/densenet/fig1.png)

DenseNets have many advantages :

* They require substantially fewer parameters and less computation to achieve state-of-the-art performances ;
* They naturally integrate the properties of identity mappings, deep
supervision, and diversified depth ;
* They are very compact, since they allow feature reuse.

## Architecture

Dense blocks are chained together using transition layers. The transition layers have the purpose of reducing the spatial extents of their inputs, and can also "compress" the feature maps to a smaller number. Figure 2 shows an example DenseNet architecture.

![](/deep-learning/images/densenet/fig2.png)

## Results

Different DenseNet architecture have stolen the state of the art from FractalNet and Wide ResNet for CIFAR-10, CIFAR-100 and Street View House Numbers. Moreover, DenseNets are on par with ResNets on the ImageNet classification task, using half the number of parameters (see section 4.4 for details).

![](/deep-learning/images/densenet/table2.png)

#### Efficiency on CIFAR-10

As you can see in the following figure, DenseNets have impressive parameter efficiency and computation efficiency. On the right, see the training and testing curves of a ResNet with **10.2M** parameters and a DenseNet with only **0.8M** parameters.

![](/deep-learning/images/densenet/fig4.png)

#### Efficiency on ImageNet

As with CIFAR-10, DenseNets are very efficient for the ImageNet classification task.

![](/deep-learning/images/densenet/fig3.png)

## Feature Reuse

A very important aspect of DenseNets is feature reuse inside dense blocks. Below, see the average absolute filter weights of convolutional layers in a trained DenseNet. The color of pixel $$ (s, l) $$ encodes the average $$ L1 $$ norm (normalized by the number of input feature maps) of the weights connecting layer $$ s $$ to layer $$ l $$ within a dense block. The first row encodes the weights of the connections to the input layer of the block.

![](/deep-learning/images/densenet/fig5.png)