---
layout: review
title: Full Resolution Image Compression with Recurrent Neural Networks
tags: deep-learning image-compression rnn
cite:
    authors: "G. Toderici, D. Vincent, N. Johnston, S. Hwang "
    title:   "Full Resolution Image Compression with Recurrent Neural Networks"
    venue:   "CVPR 2017"
pdf: "https://arxiv.org/pdf/1608.05148.pdf"
---

# Idea
  Use RNN-based encoder/decoder to do image compression for different compression rates with a single model. The encoder is followed by a binarizer (Entropy coding binary PixelRNN) that stores the data with a minimal number of bits, so that the decoder can later on retrieve a lossy compression of the original image. The RNN architecture allows to control the amount of information: it increases at each iteration and a trade-off can be found between a stronger compression and a better reconstruction.
   
# Method
Encoding network → binarizer → decoding network (E and D are RNN) Optimize them altogether on the residual error.

One shot: est(xt) = Dt(B(Et(X-est(xt-1)) each successive iteration has access to more bits generated by the encoder => better reconstruction

Additive reconstruction: est(xt) += est(xt-1)

# Application
Tested on : Kodak uncompressed dataset (thumbnail sized images) / web images. Images with the worst compression ratio (High entropy) with PNG are sorted out 
with : LSTM, Associative LSTM, Residual GRU (one shot networks and additive reconstruction networks).

The metrics used for image compression try to reflect the human eye sensitivity (Peak Signal to Noise Human Visual System, Multi-scale Structural Similarity) but nothing compares to visual impression so additive materials are given at [sup]("https://storage.googleapis.com/compression-ml/residual_gru_results/supplemental.pdf").

Depending on metrics, the one shot version of LSTM and residual GRU appear to be the most promising architectures. All models benefit from the entropy coding layer.

Even without entropy coding, this is the first architecture able to outperform JPEG compression.

[code for the best residual GRU and entropy coder]("https://github.com/tensorflow/models/tree/master/compression").

Architecture :
![](/deep-learning/images/compressionrnn/arch.png)

Visual examples:
![](/deep-learning/images/compressionrnn/ex.png)





